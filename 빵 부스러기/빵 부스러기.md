# 🍞 빵 부스러기
>개발 관련 학습 중 하나의 글로 작성하기엔 짧고, <br/>
>버리기엔 아까운 부스러기 정보들을 모아두는 곳. <br/> <br/>
>임시로 작성한게 완성 되면 빵이 되어 나가는 것이다. <br/> 
>1개, 2개, 3개, ... 부스러기 들을 모아 두고 점점 정리해 가는 것이다. <br/>
>정리가 된것은 🍞이 되어 해결.
> ***



<br/>

## 목차


---

## 예상 질문.

- `자기소개 부탁드립니다.`
    - 저는 네트워크 개선과 AI 응답 문제 해결을 중심으로 성능 개선을 경험한 김민석입니다.
    프로젝트를 통해 데이터 전송량 감소, 응답 시간 단축, 오류 처리 방식 개선 같은 문제를 해결해왔습니다. 이 과정에서 얻은 기술적 경험을 활용하여 더 나은 서비스를 만드는 데 기여하고 싶습니다.
- `우리 회사에 지원하게 된 계기`
    - 이스트소프트는 다양한 도메인에서 기술을 활용하여 혁신적인 서비스를 만들어온 회사라고 생각했습니다. 특히, AI 기술을 기반으로 데이터 처리 방식과 응답 품질을 개선하는 프로젝트를 진행해온 제 경험을 이스트소프트의 서비스 개발에 적용하고 싶다고 생각해 지원하게 되었습니다.
- `지원한 직무가 ai 개발자인데, ai 개발자로써 어떤 일을 할 것 같나요?`
    - 저는 AI 백엔드 개발자로서 주로 AI 엔진을 비동기적으로 처리하여 사용자에게 빠르게 응답을 전달하는 일을 하게 될 거라고 생각했습니다. 특히, 이스트소프트의 엘런 서비스를 사용해보면서 엘런이 여러 AI 엔진을 활용해 응답을 받아 전달하는 방식이라고 느꼈습니다. 딥시크 같은 엔진도 사용하는 걸로 보아, 다양한 AI 모델의 응답을 효과적으로 관리하고 레이턴시를 줄이는 것이 AI 백엔드 개발자의 목표라고 생각했습니다. 또한, 저는 이력서 분석 서비스 프로젝트에서 AI 응답의 안정성과 성능을 개선하며 데이터를 효율적으로 처리하는 경험을 했기 때문에 이 직무에 적합하다고 판단하여 지원했습니다.
- `비전공인데, 개발자로 지원한 계기가 무엇인가?`
    - "내가 하는 일이 다른 사람들에게 도움이 될 수 있을까?"라는 고민을 자주 했습니다.
    연탄 나르기 봉사활동을 하면서 작은 노력으로도 누군가에게 실질적인 도움이 될 수 있다는 걸 깨달았습니다. 하지만 봉사는 시간과 거리의 제약이 있었고, 더 많은 사람에게 도움을 주기에는 한계가 있었습니다. 
    그러다 프로그래밍을 배우면서 기술이야말로 물리적인 제약을 넘어서 더 많은 사람들에게 도움이 될 수 있는 강력한 도구라는 걸 알게 되었습니다.
    특히 백엔드 개발은 데이터를 효과적으로 처리하고 사용자에게 필요한 정보를 빠르게 제공하는 중요한 역할을 담당한다는 점에서 매력을 느꼈습니다.
    이력서 분석 서비스 프로젝트를 통해 데이터 전송량을 줄이고 응답 시간을 단축하며 문제를 해결하는 경험을 하면서 백엔드 개발자로서의 길을 확신하게 되었습니다.
    저는 이러한 경험을 바탕으로 더 많은 사람들에게 도움이 되는 서비스를 만들고 싶어 백엔드 개발자로 지원하게 되었습니다.
- `공백기가 좀 있으신데, 답변 해주실 수 있으실까요?`
    - 초반에는 컴퓨터 분야에 아는 지인도 없고, 물어볼 사람도 없어서 혼자 개발 책을 사서 공부를 시작했습니다. 하지만, 처음 자바를 배울 때 코드의 개념을 이해하지 못하고 단어 하나하나를 외우는 방식으로 공부를 했습니다. 이 방식은 효율적이지 않았고, 돌아서면 금방 잊어버리기를 반복하면서 "아직 익숙하지 않아서 그런거겠지" 하는 믿음 때문에 시간이 많이 소요되었습니다. 그러다 보니 제 자신에게 재능이 없다고 생각하며 좌절하는 시간이 길어졌습니다.
        
        그렇게 방황하던 중, 사람들은 어떻게 개발 공부를 하는지 검색하기 시작했고, 기술 블로그를 찾아보게 되었습니다. 블로그에 기재된 이메일로 제 상황과 공부 방식에 대해 장문의 이메일을 보내며 조언을 구했습니다. 그중 한 분이 커피챗을 요청해 주셨고, 그 자리에서 제가 공부 방향과 방법을 잘못 잡고 있었다는 것을 알게 되었습니다.
        
        이 경험을 통해 저에게 맞는 공부 방법과 효율적으로 학습하는 방식을 찾을 수 있었습니다. 많은 시행착오를 겪으며 이제는 성과를 낼 수 있는 학습 방법을 잘 이해하게 되었고, 번아웃이 왔을 때 어떻게 대처해야 하는지도 알게 되었습니다. 지금은 옆에서 도와주신 지인들 덕분에 터득한 방식으로 꾸준히 학습하며 지속적으로 성장하고 있습니다.
        
- `자신 있는 프로젝트 하나 설명해주세요.`
    - 제가 진행한 프로젝트는 AI 기반 이력서 분석 서비스입니다. 이 프로젝트는 사용자가 업로드한 이력서를 분석하여 피드백을 제공하는 서비스로, 주로 PDF 파일을 처리하고 AI 응답의 불안정성을 개선하는 데 집중했습니다.
    중요하게 생각한 부분은 서비스의 응답 속도와 정확도를 동시에 개선하는 방법을 찾는 것이었습니다. 구체적인 작업 내용으로는 응답 시간 개선, AI 분석 정확도 향상, AI 응답 안정성 개선에 대해서 작업했습니다.
    성능 개선과 정확도 향상 사이에서 발생하는 트레이드오프를 해결하기 위해, 정확도를 우선하는 서빙 방안을 구축하는 방식을 선택했습니다. 또한, 문제의 원인을 분석했습니다.
- `프로젝트에서 무엇을 했는지, 자세히 설명해주세요`
    - **응답 시간 개선:**
        
        처음 문제를 인식한 것은 AI 처리 시간이 7초밖에 걸리지 않는데도 전체 응답 시간이 평균 25초로 느리다는 점이었습니다. 이를 개선하기 위해 PDF를 TXT로 변환하여 데이터 크기를 5MB에서 평균 200KB로 줄이고, Gzip 압축을 적용하여 전송량을 80% 줄였습니다. 그 결과, 응답 시간을 25초에서 11초로 개선했습니다.
        
    - **AI 분석 정확도 향상:**
        
        기존 방식의 한계를 분석한 후, Chain-of-Thought과 Few-shot Learning을 적용하여 AI의 사고 과정을 개선했습니다. 이를 통해 AI 분석 정확도가 37% 향상되었지만, 응답 속도가 평균 2~3초 증가하는 트레이드오프가 발생했습니다.
        
    - **AI 응답 안정성 개선:**
        
        AI 응답 형식을 정형화하고, 11개의 주요 실패 사례를 분류하여 문서화했습니다. 또한, 유효성 검증 및 자동 수정 기능을 추가하여 JSON 파싱 실패율을 83% 감소시켰고, 자동 재시도 로직을 개선하여 API 재시도 횟수를 65% 줄이며 서비스 안정성을 75%에서 99%로 향상시켰습니다.
        
- `Gzip을 어떻게 성능개성을 했나요?`
    - Gzip을 적용하여 성능을 개선한 방법은 PDF를 TXT로 변환한 후 Gzip으로 압축하여 데이터 크기를 줄이는 것입니다. TXT로 변환하면 크기가 5MB에서 3KB로 감소하여 네트워크 전송량을 최대 80% 줄였습니다. 이 방식으로 응답 시간을 13초에서 11초로 단축했습니다.
- `압축을 했는데, 시간이 주는 이유는 어떤 이유 였을까요?`
    - 압축을 했는데 시간이 줄어든 이유는 데이터 크기가 줄어들어 네트워크 전송 시간이 감소했기 때문입니다. 압축 과정은 추가 연산이 필요하지만, 압축된 데이터의 크기가 작아지면서 전송 시간이 더 많이 단축됩니다. 특히, Gzip 압축을 적용하면 데이터 크기가 최대 80% 줄어들기 때문에 네트워크 전송 시간이 크게 줄어들어 전체 응답 시간이 단축되었습니다.
- `Gzip 압축 방식에 대해 아시나요?`
    - Gzip 압축은 **DEFLATE 알고리즘(LZ77 + 허프만 코딩)**을 사용하여 중복 패턴을 참조 포인터로 대체하고, 자주 나타나는 데이터를 짧게 표현하여 데이터 크기를 줄이는 방식입니다.
- `LZ77 알고리즘 + 허프만에 대해 설명해주세요.`
    - LZ77 알고리즘은 중복 데이터를 탐색하여 이전 위치를 참조 포인터로 저장해 압축하고, 허프만 코딩은 데이터의 빈도수를 기반으로 자주 나타나는 데이터를 더 짧은 비트로 표현하여 압축률을 극대화하는 방식으로, Gzip은 LZ77로 패턴을 압축한 후 허프만 코딩으로 비트 단위 압축을 추가하여 데이터 크기를 효과적으로 줄입니다.
- `Chain-of-Thought와 Few-shot Learning 각각에 대해서 설명해주세요.`
    - Chain-of-Thought은 AI가 문제를 논리적으로 접근할 수 있도록 사고 과정을 단계별로 유도하여 응답의 정확성과 일관성을 높이는 방식이고, Few-shot Learning은 몇 가지 예제를 제공하여 AI가 학습 없이도 유사한 문제를 해결하도록 패턴을 학습시키는 방식으로, 두 방법 모두 AI의 성능을 개선하는 데 사용됩니다.
- `Chain-of-Thought과 Few-shot Learning을 적용하기 전과 적용 후의 분석 정확도를 어떻게 비교하셨나요?`
    - Chain-of-Thought과 Few-shot Learning을 적용하기 전과 후의 분석 정확도를 **구체성, 직무 연관성, 개선 방향 명확성, 예시 포함 여부**의 기준으로 평가했습니다. 각 응답을 점수화하여 정확도를 수치로 계산했고, 기존 방식(33.33%)과 개선 방식(70.64%)을 비교하여 정확도가 37.3% 향상된 것을 확인했습니다.
- `정확도 37프로가 된 이유가 궁금합니다. 어떻게 저런 수치가 나왔나요?`
    - 정확도가 37.3% 향상된 이유는 Chain-of-Thought과 Few-shot Learning을 적용하여 AI 피드백의 품질을 개선했기 때문입니다. 정확도는 AI가 생성한 피드백 중에서 유용한 피드백의 비율로 계산했으며, 예를 들어 총 30개의 피드백 중 21개가 유용할 경우 정확도는 70%로 계산됩니다. 유용한 피드백은 **구체적인 내용 포함 여부, 직무와의 연관성, 개선 방향의 명확성, 예시 포함 여부**의 4가지 기준으로 평가했습니다. 기존 방식의 정확도는 33%였지만, Chain-of-Thought과 Few-shot Learning을 적용한 방식은 70%로 평가되었습니다. 이 결과, 정확도가 37% 향상되었다는 결론을 얻을 수 있었습니다.
